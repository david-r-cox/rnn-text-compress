{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![MAA MathFest 2016](http://www.maa.org/sites/default/files/images/mathfest/2016_mathfest_banner_with_dates.gif \"MAA MathFest 2016\")\n",
    "Syntactically Informed Text Compression with Recurrent Neural Networks\n",
    "======\n",
    "## David Cox, Dr. J. Maurice Rojas\n",
    "## Texas A&M University\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# About Me\n",
    "\n",
    "* Texas A&M University class of 2016\n",
    "* Computer science, mathematics\n",
    "* Malware analysis at Palo Alto Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Texas A&M University](http://brandguide.tamu.edu/downloads/logos/TAMU-logos-rgb/TAM-\n",
    "Wordmark/TAM-Wordmark.png \"Texas A&M University\")\n",
    "\n",
    "\n",
    "* This is my first time speaking at a conference.\n",
    " * This is also my first time attending a math conference.\n",
    " * (bear with me!)\n",
    "* My slides were generated using IPython Notebook.\n",
    " * If you like the format, now you know.\n",
    " * If you hate it... sorry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Project Overview\n",
    "\n",
    "We explored using recurrent neural networks to construct language models for text compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Dr. Mahoney](https://cs.fit.edu/~mmahoney/matt.jpg \"Dr. Mahoney\")\n",
    "We built upon work done by Dr. Mahoney at Florida Tech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* He used a 4 x 1,000,000 x 1 fully connected neural network optimized for single-pass training to produce bit-level probability estimates for text sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In his publication, Dr. Mahoney notes that this model does not take advantage of syntactic or semantic relationships present in natural language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* He cites the ability to exploit these relationships as the motivation behind using neural networks in the first place. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We utilized Google's state-of-the-art natural language parser to obtain highly accurate syntactic information in the form of part of speech tags. \n",
    "![Parsey McParseFace](https://2.bp.blogspot.com/-fqtmVS97tOs/VzTEAI9BQ8I/AAAAAAAAA_U/xPj0Av64sGseS0rF4Z1BbhmS77J-HuEvwCLcB/s1600/image04.gif \"Parsey McParseface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Google's parser is named Parsey McParseface, taking after the internet meme \"Boaty McBoatface\"\n",
    "\n",
    "![Boaty](https://www.dailywire.com/sites/default/files/maxresdefault_0.jpg \"Boaty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We also used a recurrent neural network architecture better suited to modeling sequences of data, such as text.\n",
    "\n",
    "![Recurrent Neural Network](https://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg \"Recurrent Neural Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### We found that the addition of syntactic information improved the accuracy of our model by an average of 5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Accuracy](./report/images/tags_vs_no-tags.png \"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bottom line\n",
    "\n",
    "* Providing a neural network with more, valid information results in a more accurate model.\n",
    "* In our case, the model can then be used for text compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Computer science disclaimer\n",
    "\n",
    "Data compression and neural networks have roots in both mathematics and computer science. We've tried to make this talk as accessible possible, but please ask for further explanation at the end of the talk if something isn't clear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Data Compression\n",
    "\n",
    "* Data compression is the process of encoding a message with the goal of reducing the number of symbols required to represent the message. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Compression operates in two steps\n",
    "* Modeling\n",
    " * Modeling is the process of generating an estimate for the expected probability of symbols in the input.\n",
    " * Accurate probability estimates result in efficient compression. \n",
    "* Coding\n",
    " * Coding is the process of translating probability estimates to a sequence of symbols. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modern coding methods are theoretically optimal, while modeling is considered an \"unsolvable problem\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Consider an optimal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* An optimal model could predict the next symbol in an arbitrary sequence with 100% accuracy.\n",
    "* A compression scheme based upon this model would be able to recursively compress its own output to zero bytes.\n",
    " * This would violate Kolmogorov's principle of string complexity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The current best approach to improving compression is to construct domain-specific models and interface them with a near-optimal coding method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Arithmetic coding is a popular, near-optimal coding method that operates by representing a sequence of probabilities as a fractional number in the interval [0,1).\n",
    "\n",
    "### Consider the following example\n",
    "* Let $M$ be a message composed of symbols: [C, O, D, E, !]\n",
    "* Let $S$ be an alphabet containing the symbols {A, B, C, D, E, O, !}\n",
    "* The following table contains an arbitrary, fixed probability model for the symbols in $S$.\n",
    "\n",
    "| Symbol        | Probability   | Range      |\n",
    "| :-----------: |:-------------:| :-----     |\n",
    "| A             | 0.3           | [0, 0.3)   |\n",
    "| B             | 0.2           | [0.3, 0.5) |\n",
    "| C             | 0.2           | [0.5, 0.7) |\n",
    "| D             | 0.1           | [0.7, 0.8) |\n",
    "| E             | 0.1           | [0.8, 0.9) |\n",
    "| O             | 0.05          | [0.9, 0.95)|\n",
    "| !             | 0.05          | [0.95 ,1)  | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### We encode $M$ by reducing the range of our subinterval from its initial range of [0,1) for each symbol as shown:\n",
    "\n",
    "| Step          | After         | Range               |\n",
    "| :-----------: |:-------------:| :------------------:|\n",
    "| 0             | $\\emptyset$   | [0, 1)              |\n",
    "| 1             | C             | [0.5, 0.7)          |\n",
    "| 2             | O             | [0.680, 0.690)      |\n",
    "| 3             | D             | [0.678, 0.688)      |\n",
    "| 4             | E             | [0.6878, 0.6879)    |\n",
    "| 5             | !             | [0.687895, 0.68790) |\n",
    "| Done          | CODE!         | 0.687895            |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In this example the output ended up requiring more symbols than the input, which is typically not the case. \n",
    "* This was caused by using an arbitrary probability model. Our unfortunate example highlights the need for accurate language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### We just used a static model to perform arithmetic coding, but models can also be dynamic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Dynamic models allow for probability estimates to be updated based on the symbols or characters that have been processed. \n",
    "* Neural networks can be used as dynamic models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Recurrent neural networks are a class of neural networks well suited for modeling sequential data such as audio or text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* RNN's excel in these domains due to memory provided by a recurrence in their hidden layers.\n",
    "* This memory allows for the representation of contextual dependencies over arbitrary time intervals. \n",
    "* We can represent the hidden state of a RNN with a simple set of recurrence equations:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "net_j(t) &= \\sum_i^nx_i(t)v_{ji} + \\sum_h^m y_h(t-1)u_{jh} + \\theta_j\\\\\n",
    "y_j(t) &= f\\left(net_j(t)\\right)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $y_j(t)$ is the output of the hidden state at time $t$ and $y_h(t-1)$ is the hidden state from the previous time interval. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Vectors $\\mathbf{x}, \\mathbf{V}, \\mathbf{U}$ and $\\mathbf{W}$ are weights:\n",
    " * input\n",
    " * input-hidden\n",
    " * hidden-hidden\n",
    " * hidden-out\n",
    "* Each layer is assigned an index variable\n",
    " * $k$ for output nodes\n",
    " * $j,h$ for hidden nodes\n",
    " * $i$ for input nodes\n",
    "* The functions $f$ and $g$ are differentiable, nonlinear activation functions such as the sigmoid or hyperbolic tangent function. \n",
    "* $\\theta_j$ is a bias.\n",
    "* The output state $y_k(t)$ can be computed as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "net_k(t) &= \\sum_j^m y_j(t)w_{kj} + \\theta_k\\\\\n",
    "y_k(t) &= g\\left(net_k(t)\\right)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "All together, we see that a single forward pass through the network can be calculated with the following recurrence:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_k(t) &= g\\left( \\sum_j^m \\left( f\\left(  \\sum_i^nx_i(t)v_{ji} + \\sum_h^m y_h(t-1)u_{jh} + \\theta_j  \\right) \\right) w_{kj} + \\theta_k \\right)\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Backpropagation Through Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### To allow for learning over arbitrary intervals, error values must be backpropagated through time. \n",
    "\n",
    "We use the cross entropy error function in our model, defined as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "C &= \\frac{1}{2} \\sum_p^n H(d,y)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for the $p$th sample in the training set of length $n$ and the cross entropy function $H(p,q)$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(p,q) &= - \\sum_x p(x) \\log q(x)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "All together, our error function is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "C &= \\frac{1}{2} \\sum_p^n \\left( - \\sum_k^m d_{pk} \\log (y_{pk}) \\right)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for $d$, the desired output of $m$ output nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Weight updates are proportional to the negative cost gradient with respect to the weight that is being updated, scaled by the learning rate, $\\eta$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Delta w &= - \\eta \\frac{\\partial C}{\\partial w}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can then compute the output error, $\\delta_{pk}$ and hidden error, $\\delta_{pj}$, which can be backpropagated through time to obtain the error of the hidden layer at the previous time interval.\n",
    "\n",
    "Indices $h$ and $j$ are for nodes sending and receiving the activation, respectively.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta_{pk} &= \\frac{\\partial C}{\\partial y_{pk}} \\frac{\\partial y_{pk}}{\\partial net_{pk}}\\\\\n",
    "\\delta_{pj} &= - \\left( \\sum_k^m \\frac{\\partial C}{\\partial y_{pk}} \\frac{\\partial y_{pk}}{\\partial net_{pk}} \\frac{\\partial net_{pk}}{y_{pj}}\\right) \\frac{\\partial y_{pj}}{\\partial net_{pj}}\\\\\n",
    "&= \\sum_k^m \\delta_{pk}w_{kj}f'(y_{pj}) \\\\\n",
    "\\delta_{pj}(t-1) &= \\sum_h^m \\delta_{ph}(t)u_{hj}f' \\left( y_{pj}(t-1) \\right)\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gated Recurrent Units\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### While recurrent neural networks are more suited to this problem than the feedforward networks used by Dr. Mahony, they come with a significant drawback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### When backpropagation is done over many timer intervals, error gradients can either vanish or \"explode\". This prevents the model from learning and reduces accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A popular solution to this problem is the use of Gated Recurrent Units (GRUs).\n",
    "\n",
    "* Networks of gated recurrent units allow for modeling of multiple long-term dependencies at arbitrary time scales. \n",
    " * They achieve this by allowing gates to reset themselves.\n",
    "* A single GRU consists of a hidden state along with reset and update gates. \n",
    " * When the reset gate, $r_j$ is closed $(r_j = 0)$, the value of the GRU's previous hidden state is ignored.\n",
    " * This resets the unit.\n",
    " \n",
    "The value of the reset gate is computed as:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "r_j = \\sigma \\left( v_{jr} x_i + u_{jr} y_{h}(t-1)\\right)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for the sigmoid activation function $\\sigma(t) = \\left(1+e^t\\right)^{-1}$, the unit's input and previous hidden state, $x_i$ and $y_{h}(t-1)$, respectively. The weight matrices $\\mathbf{V}$ and $\\mathbf{U}$ follow from our previous equations.\n",
    "\n",
    "The update gate $z_j$ is similar:\n",
    "\n",
    "\\begin{align*}\n",
    "z_j = \\sigma \\left( v_{jz} x_i + u_{jz} y_{h}(t-1)\\right)\\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$\\widetilde{y}$ is pronounced \"y wiggle\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The new hidden state, $\\widetilde{y}_{j}(t)$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\widetilde{y}_{j}(t) = \\tanh \\left( v_{j} x_{j} + u_j\\left(r_j y_{h}(t-1)\\right) \\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "*Note the role of the reset gate in the calculation of the new hidden state.*\n",
    "\n",
    "\n",
    "Finally, the unit's activation function, $y_{j}(t)$ can be calculated as a linear interpolation between the previous and current states:\n",
    "\n",
    "\\begin{align*}\n",
    "y_{j}(t) = z_j y_{h}(t-1) + (1-z_j)\\widetilde{y}_{j}(t) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The output of a single forward pass in a single layer GRU network can be represented using notation from the previous simple recurrent model:\n",
    "\n",
    "\\begin{align*}\n",
    "net_k(t) &= \\sum_j^m y_j(t)w_{kj} + \\theta_k\\\\\n",
    "  &= \\sum_j^m \\left( z_j y_{h}(t-1) + (1-z_j)\\widetilde{y}_{j}(t)\\right) w_{kj} + \\theta_k\\\\\n",
    "y_k(t) &= g\\left(net_k(t)\\right)\\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Architecture\n",
    "\n",
    "### Our background information followed a series of problems.\n",
    "* The need for an effective language model was addressed.\n",
    " * An ideal neural network architecture was discussed,\n",
    "   * and an improved hidden unit was selected.\n",
    "   \n",
    "This part of the talk will address the issue of improving upon a vanilla GRU network architecture that operates solely on character sequences. The improvements discussed occur at a higher level of abstraction than the gate level architectures previously described, as we are seeking to build a practical model rather than propose a new recurrent unit architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Jumping right in: Implementation with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "char_encoder = Sequential(name='char_encoder')\n",
    "pos_encoder  = Sequential(name='pos_encoder')\n",
    "char_encoder.add(\n",
    "        GRU(\n",
    "            output_dim=256,\n",
    "            return_sequences=True,\n",
    "            input_shape=(maxlen, len(chars)),\n",
    "            consume_less='gpu',\n",
    "            )\n",
    "        )\n",
    "char_encoder.add(Dropout(0.1))\n",
    "pos_encoder.add(\n",
    "        GRU(\n",
    "            output_dim=49,\n",
    "            return_sequences=True,\n",
    "            input_shape=(maxlen, len(tags)),\n",
    "            consume_less='gpu',\n",
    "            )\n",
    "        )\n",
    "pos_encoder.add(Dropout(0.1))\n",
    "decoder = Sequential(name='decoder')\n",
    "decoder.add(Merge([char_encoder, pos_encoder], mode='concat'))\n",
    "decoder.add(\n",
    "        GRU(\n",
    "            output_dim=305,\n",
    "            return_sequences=False,\n",
    "            consume_less='gpu',\n",
    "            )\n",
    "        )\n",
    "decoder.add(Dense(len(chars), activation='relu'))\n",
    "decoder.add(Dense(len(chars), activation='softmax'))\n",
    "decoder.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Model Architecture](./report/model_architecture.png \"Model Architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Notation used:\n",
    "\n",
    "| Layer                                    | Description           | \n",
    "| :----------------------------------------|:----------------------| \n",
    "| $\\mathbf{x}^{\\langle c \\rangle}$      | Character input layer | \n",
    "| $\\mathbf{x}^{\\langle p \\rangle}$      | POS input layer       | \n",
    "| $\\mathbf{y}^{\\langle c \\rangle}$      | GRU layer (character) | \n",
    "| $\\mathbf{y}^{\\langle p \\rangle}$      | GRU layer (POS)       | \n",
    "| $\\mathbf{\\Xi}^{\\langle c p \\rangle}$  | Dropout layer         |  \n",
    "| $\\mathbf{\\Psi}^{\\langle c,p \\rangle}$ | Merge layer           | \n",
    "| $\\mathbf{y}^{\\langle \\Psi \\rangle}$   | GRU layer (merged)    | \n",
    "| $\\mathbf{y}^{\\langle D1 \\rangle}$     | Dense layer: RELU     | \n",
    "| $\\mathbf{y}^{\\langle D1 \\rangle}$     | Dense layer: Softmax  | \n",
    "| $\\mathbf{y}^{\\langle out \\rangle}$    | Network output        | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The character input layer, $\\mathbf{x}^{\\langle c \\rangle}(t)$, is a $40 \\times 256$ one-hot representation of forty character sequences. This layer is paralleled by a second input layer containing part of speech information obtained from SyntaxNet. \n",
    "\n",
    "The part of speech tag (POS) input layer, $\\mathbf{x}^{\\langle p \\rangle}(t)$ is a $40 \\times 49$ one-hot representation of part of speech tag sequences, each of which correspond to the character at the same respective index in the other input layer.\n",
    "\n",
    "One-hot encoding is a sparse way to represent information in which a single bit in an array is \"hot\" (1), with the rest being low (0).\n",
    "![one-hot](https://cd8ba0b44a15c10065fd-24461f391e20b7336331d5789078af53.ssl.cf1.rackcdn.com/graphlab.vanillaforums.com/editor/v3/b3s53janx6l2.png \"one-hot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "GRU layers $\\mathbf{y}^{\\langle c \\rangle}(t)$ and $\\mathbf{y}^{\\langle p \\rangle}(t)$ are also parallel. We will use the notation $\\mathbf{y}^{\\langle c|p \\rangle}(t)$ when discussing separate but identical operations to both layers. Our implementation utilizes the hard (linearly approximated) sigmoid function in place of the standard logistic sigmoid as the GRU's inner activation function in order to reduce computational requirements. The outer activation, $g$ is the hyperbolic tangent function applied element-wise for each node in the layer. \n",
    "\n",
    "A forward pass through $\\mathbf{y}^{\\langle c \\rangle}(t)$ and $\\mathbf{y}^{\\langle p \\rangle}(t)$ is calculated as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "net^{\\langle c|p \\rangle}_j(t) &= \\sum_i^n \\left[ \\left( z_i y_{h}(t-1) + (1-z_i)\\widetilde{y}_{i}(t)\\right) v_{ji} + \\theta_j\\right]^{\\langle c|p \\rangle}\\\\\n",
    "y^{\\langle c|p \\rangle}_j(t) &= f\\left(net^{\\langle c|p \\rangle}_j(t)\\right)\\\\\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$\\Xi$  pronounced \"ksee\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dropout layers are used to prevent overfitting.\n",
    "\n",
    "$\\mathbf{\\Xi}^{\\langle c \\rangle}(t)$ and $\\mathbf{\\Xi}^{\\langle p \\rangle}(t)$ are applied to $\\mathbf{y}^{\\langle c \\rangle}(t)$ and $\\mathbf{y}^{\\langle p \\rangle}(t)$, respectively. The output of the dropout layers is a replica of the input, with the exception that output from a fractional number of nodes, randomly selected with probability $\\rho$ is pinned to zero. \n",
    "\n",
    "After applying dropout, the state of the model is as follows:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Xi^{\\langle c|p \\rangle}_j(t) &= \\xi \\left(y^{\\langle c|p \\rangle}_j(t)\\right)\\\\\n",
    "\\text{where } \\xi(x) &= \\begin{cases}\n",
    "      0 & \\text{ with probability } \\rho\\\\\n",
    "      x & \\text{ otherwise}\n",
    "   \\end{cases}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Model Architecture](./report/model_architecture.png \"Model Architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A merge layer, $\\mathbf{\\Psi}^{\\langle c,p \\rangle}(t)$ is applied to the output of the two dropout layers. This layer is a simple vector concatenation, represented here by the $||$ operator.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{\\Psi}^{\\langle c,p \\rangle}(t) = \\mathbf{\\Xi}^{\\langle c \\rangle}(t) \\; || \\; \\mathbf{\\Xi}^{\\langle p \\rangle}(t)\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Model Architecture](./report/model_architecture.png \"Model Architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The merged output feeds into a final GRU layer, $\\mathbf{y}^{\\langle m \\rangle}(t)$ followed by two fully connected layers,  $\\mathbf{y}^{\\langle D1 \\rangle}(t)$ and  $\\mathbf{y}^{\\langle D2 \\rangle}(t)$ to produce the network output $\\mathbf{y}^{\\langle out \\rangle}(t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "net^{\\langle \\Psi \\rangle}_j(t) &= \\sum_i^n \\left[ \\left( z_i y_{h}(t-1) + (1-z_i)\\widetilde{y}_{i}(t)\\right) w_{ji} + \\theta_j\\right]^{\\langle \\Psi \\rangle}\\\\\n",
    "y^{\\langle \\Psi \\rangle}_j(t) &= f \\left( net^{\\langle \\Psi \\rangle}_j(t) \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "net^{\\langle D1 \\rangle}_j(t) &= \\sum_j^m \\left[y_j(t)w_{j} + \\theta_j \\right]^{\\langle \\Psi \\rangle} \\\\\n",
    "y^{\\langle D1 \\rangle}_j(t) &= \\text{ReLU}\\left( net^{\\langle D1 \\rangle}_j(t) \\right) \\\\\n",
    "\\text{where ReLU}(x) &= \\max(0,x)\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "net^{\\langle D2 \\rangle}_j(t) &= \\sum_j^m \\left[y_j(t)w_{j} + \\theta_j \\right]^{\\langle D1 \\rangle} \\\\\n",
    "y^{\\langle D2 \\rangle}_j(t) &= \\text{softmax}\\left( net^{\\langle D2 \\rangle}_j(t) \\right) \\\\\n",
    "\\text{softmax}(x) &= e^x \\left(\\sum_{n}^m e^{x_n}\\right)^{-1}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "y^{\\langle out \\rangle}_k(t) &= y^{\\langle D2 \\rangle}_j(t)\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Model Architecture](./report/model_architecture.png \"Model Architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To keep calculation simple, we've been operating on individual neural units. As we've reached the output layer, it's important to remember that we're working with vectors:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{y}^{\\langle out \\rangle}(t) &= \\left[y^{\\langle out \\rangle}_0(t), ..., y^{\\langle out \\rangle}_k(t)\\right] \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now see why the softmax activation function is critical to the model -- the network's output always sums to one and is a valid representation of probability estimates for each character:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sum_{i=0}^j \\left[ y^{\\langle out \\rangle}_i(t), ..., y^{\\langle out \\rangle}_k(t) \\right] &= 1\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\"Differen-TA-tion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation for a model of this complexity is not an easy task!\n",
    "\n",
    "Fortunately, automatic differentiation frees us of this burden. This calculation is handled by Theano, the computation library wrapped by Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training was performed on books obtained from Project Gutenberg. A single book was used for each model constructed. Models were trained for a minimum of 700 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Amazon ```g2.2xlarge``` EC2 instances were used to perform training and evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Each epoch took approximately 230 seconds, equating to roughly 48 hours of computation per document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "###  We've provided a preconfigured AMI for those wishing to verify or expand upon our results without going through the trouble of resolving software dependencies. The AMI is publicly available as ```ami-2c3a7a4c```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### To quantify the effect of part of speech information, models for *Pride and Prejudice* were trained with and without part of speech tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "E\\left[\\left( \\frac{\\partial C}{\\partial w} \\right) ^2\\right]^{\\langle t \\rangle}\\!\\!\\!\\!\\!\\!\\! &= 0.9E\\left[\\left( \\frac{\\partial C}{\\partial w} \\right) ^2\\right]^{\\langle t-1 \\rangle}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! + 0.1\\left[\\left( \\frac{\\partial C}{\\partial w} \\right) ^2\\right]^{\\langle t \\rangle}\\\\\n",
    "\\theta^{\\langle t + 1\\rangle} &= \\theta^{\\langle t \\rangle} - \\frac{\\eta}{\\sqrt{E\\left[\\left( \\frac{\\partial C}{\\partial w} \\right) ^2\\right]^{\\langle t \\rangle}} + \\epsilon}\\left[\\frac{\\partial C}{\\partial w}\\right]^{\\langle t \\rangle}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Results\n",
    "\n",
    "### All models converged to a high level of accuracy within the training window. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Tags vs no tags](./report/images/tags_vs_no-tags.png \"Tags vs no tags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Results](./report/images/results1.png \"Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Several interesting things to note\n",
    "* The *Tale of Two Cities* model exhibited gradient instability around epoch 650\n",
    " * This is likely due to continued training after convergence.\n",
    " * GRUs improve gradient stability, but they're not perfect. \n",
    " * This is a good example of the sometimes chaotic behavior of recurrent neural networks\n",
    "* Our model effectively memorized *Alice in Wonderland*.\n",
    " * Overfitting should be avoided when producing generalized models, but it's not a huge issue here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Concluding Thoughts\n",
    "\n",
    "### Based on the accuracy of the document-specific models, we believe training an effective generalized model would be possible. \n",
    "\n",
    "### We did not attempt to train such a model due to the computational time required relative to the duration of this project. \n",
    "\n",
    "### SyntaxNet could further be utilized. Adding word dependency information would likely further improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Resources\n",
    "\n",
    "### All of our code is available at \n",
    "\n",
    "```\n",
    "https://github.com/davidcox143/rnn-text-compress\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
